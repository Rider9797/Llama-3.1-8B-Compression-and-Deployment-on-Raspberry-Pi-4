## LLM Compression & Deployment on Raspberry Pi 4 (Prune **-->** Convert into GGUF --> Quantize -- Deploy)

## Overview
This repository contains the methodology, scripts, and results for compressing Llama 3.1 8B to run efficiently on a Raspberry Pi 4 (8GB). By combining structural pruning, LoRA fine-tuning, and advanced quantization (AWQ/IMatrix) via llama.cpp, we reduced the model's memory footprint by ~80% while maintaining linguistic coherence.




## Abstract
